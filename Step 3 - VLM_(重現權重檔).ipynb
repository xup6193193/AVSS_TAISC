{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b57da3-eba5-41bf-8fc1-279f103d0440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ç’°å¢ƒè³‡è¨Š] éš¨æ©Ÿç¨®å­å·²è¨­å®šç‚º: 42\n",
      "[ç’°å¢ƒè³‡è¨Š] åŸ·è¡Œè¨­å‚™: cuda\n",
      "[ç’°å¢ƒè³‡è¨Š] GPU åç¨±: NVIDIA GeForce RTX 3080\n",
      "æœ¬æ¬¡é æ¸¬çµæœå°‡å„²å­˜æ–¼: inference_results_20250719_001815\n",
      "\n",
      "=============== å·²é‡å»ºçš„æ¡æ¨£ç­–ç•¥ ===============\n",
      "{'name': 'Last_1_to_5', 'start': -5, 'end': None}\n",
      "{'name': 'Last_6_to_10', 'start': -10, 'end': -5}\n",
      "{'name': 'Last_11_to_15', 'start': -15, 'end': -10}\n",
      "--------------------------------------------------\n",
      "\n",
      "=============== æ­¥é©Ÿ A: æº–å‚™æ¸¬è©¦è³‡æ–™ ===============\n",
      "Tokenizer å·²å¾ output_pytorch_MultiWindow_20250710_131200\\tokenizer.pkl è¼‰å…¥ã€‚\n",
      "--------------------------------------------------\n",
      "\n",
      "######################### é–‹å§‹è™•ç†ç­–ç•¥: Last_1_to_5 #########################\n",
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wolves9\\AppData\\Local\\Temp\\ipykernel_12856\\1005008649.py:210: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_1_to_5, Fold: 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:18<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_1_to_5, Fold: 2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:17<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_1_to_5, Fold: 3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:17<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_1_to_5, Fold: 4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:17<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_1_to_5, Fold: 5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:17<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç­–ç•¥ [Last_1_to_5] é æ¸¬å®Œæˆã€‚\n",
      "\n",
      "######################### é–‹å§‹è™•ç†ç­–ç•¥: Last_6_to_10 #########################\n",
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_6_to_10, Fold: 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:17<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_6_to_10, Fold: 2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:17<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_6_to_10, Fold: 3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:18<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_6_to_10, Fold: 4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:17<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_6_to_10, Fold: 5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:17<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç­–ç•¥ [Last_6_to_10] é æ¸¬å®Œæˆã€‚\n",
      "\n",
      "######################### é–‹å§‹è™•ç†ç­–ç•¥: Last_11_to_15 #########################\n",
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_11_to_15, Fold: 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:17<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_11_to_15, Fold: 2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:17<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_11_to_15, Fold: 3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:17<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_11_to_15, Fold: 4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:17<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ (vit_base_patch16_clip_224.openai) æ¶æ§‹å·²å»ºç«‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é æ¸¬ä¸­ (ç­–ç•¥: Last_11_to_15, Fold: 5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:17<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç­–ç•¥ [Last_11_to_15] é æ¸¬å®Œæˆã€‚\n",
      "\n",
      "========================= é–‹å§‹çµ‚æ¥µé›†æˆé æ¸¬ =========================\n",
      "\n",
      "âœ… é‡ç¾çš„é æ¸¬çµæœå·²å„²å­˜è‡³: inference_results_20250719_001815\\submission_reproduced.csv\n",
      "é æ¸¬åˆ†ä½ˆ:\n",
      "risk\n",
      "0    149\n",
      "1     94\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ…âœ…âœ… åŒ…å«æ©Ÿç‡çš„é æ¸¬çµæœå·²å„²å­˜è‡³: inference_results_20250719_001815\\submission_probabilities.csv\n",
      "\n",
      "ğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰é æ¸¬æµç¨‹åŸ·è¡Œå®Œç•¢ï¼ğŸ‰ğŸ‰ğŸ‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ===================================================================\n",
    "# è»Šç¦é æ¸¬æ¯”è³½ - PyTorch å¤šæ¨¡æ…‹æ¨¡å‹é æ¸¬è…³æœ¬\n",
    "# èªªæ˜: æ­¤è…³æœ¬ç”¨æ–¼è¼‰å…¥å·²è¨“ç·´çš„æ¨¡å‹ï¼Œä¸¦å°æ¸¬è©¦é›†é€²è¡Œé æ¸¬ã€‚\n",
    "# ===================================================================\n",
    "\n",
    "# --- 0. åŒ¯å…¥å¿…è¦å¥—ä»¶ ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "# ===================================================================\n",
    "#                        *** 1. è¨­å®šå€ ***\n",
    "# ===================================================================\n",
    "# !!! é‡è¦ !!!\n",
    "# æ­¤è™•çš„ Config å¿…é ˆèˆ‡æ‚¨ç”¨ä¾†è¨“ç·´çš„ VLM_5X3_2éšæ®µ(065).py è…³æœ¬ä¸­çš„è¨­å®šå®Œå…¨ä¸€è‡´\n",
    "# æ‰èƒ½æ­£ç¢ºè¼‰å…¥æ¨¡å‹æ¶æ§‹èˆ‡è™•ç†è³‡æ–™ã€‚\n",
    "class Config:\n",
    "    # --- 1. è·¯å¾‘ã€è¼¸å‡ºèˆ‡ç’°å¢ƒè¨­å®š ---\n",
    "    BASE_IMG_PATH = '.'                   # åœ–åƒè³‡æ–™å¤¾ (road, freeway) çš„æ ¹è·¯å¾‘\n",
    "    RANDOM_STATE = 42                     # å›ºå®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯¦é©—å¯é‡è¤‡\n",
    "    \n",
    "    # --- 2. å‹•æ…‹æ¡æ¨£ç­–ç•¥è¨­å®š ---\n",
    "    WINDOW_SIZE = 5                       # æ¯å€‹æ¡æ¨£çª—å£è¦åŒ…å«å¤šå°‘å¼µåœ–ç‰‡\n",
    "    NUM_WINDOWS = 3                       # ç¸½å…±è¦å»ºç«‹å¤šå°‘å€‹æ¡æ¨£çª—å£ (çµ„)\n",
    "    \n",
    "    # --- 3. è³‡æ–™è™•ç†è¶…åƒæ•¸ ---\n",
    "    MAX_IMG_SEQ_LEN = WINDOW_SIZE         # åœ–åƒåºåˆ—é•·åº¦ç¾åœ¨ç”±çª—å£å¤§å°æ±ºå®š\n",
    "    MAX_TEXT_LEN = 512                    # VLM æ–‡å­—åºåˆ—çš„æœ€å¤§é•·åº¦\n",
    "    IMG_SIZE = (224, 224)                 # åœ–åƒæ¨¡å‹çš„æ¨™æº–è¼¸å…¥å°ºå¯¸\n",
    "    VOCAB_SIZE = 8000                     # æ–‡å­—å­—å…¸å¤§å°\n",
    "    \n",
    "    # --- 4. æ¨¡å‹æ¶æ§‹è¶…åƒæ•¸ ---\n",
    "    MODEL_NAME = 'vit_base_patch16_clip_224.openai' # ä½¿ç”¨çš„é è¨“ç·´åœ–åƒæ¨¡å‹\n",
    "    EMBEDDING_DIM = 128                   # æ–‡å­—åµŒå…¥å±¤çš„ç¶­åº¦\n",
    "    TEXT_LSTM_UNITS = 64                  # è™•ç†æ–‡å­—çš„ LSTM ç¥ç¶“å…ƒæ•¸é‡\n",
    "    IMAGE_LSTM_UNITS = 64                 # è™•ç†åœ–åƒåºåˆ—çš„ LSTM ç¥ç¶“å…ƒæ•¸é‡\n",
    "    DENSE_UNITS = 128                     # èåˆå¾Œçš„å…¨é€£æ¥å±¤ç¥ç¶“å…ƒæ•¸é‡\n",
    "    DROPOUT_RATE = 0.5                    # Dropout å±¤çš„æ¯”ç‡\n",
    "    \n",
    "    # --- 5. è¨“ç·´éç¨‹è¶…åƒæ•¸ (é æ¸¬æ™‚ç”¨ä¸åˆ°ï¼Œä½†ä¿ç•™ä»¥ç¤ºå®Œæ•´) ---\n",
    "    NUM_SPLITS = 5                        # äº¤å‰é©—è­‰çš„æ‘ºæ•¸\n",
    "    BATCH_SIZE = 8                        # æ‰¹æ¬¡å¤§å° (é æ¸¬æ™‚å¯ä»¥æ ¹æ“š VRAM èª¿æ•´)\n",
    "\n",
    "# ===================================================================\n",
    "#                  *** 2. è¼”åŠ©å·¥å…·èˆ‡é¡åˆ¥å®šç¾© ***\n",
    "# ===================================================================\n",
    "# !!! é‡è¦ !!!\n",
    "# é€™äº›é¡åˆ¥èˆ‡å‡½å¼ä¹Ÿå¿…é ˆèˆ‡åŸè¨“ç·´è…³æœ¬å®Œå…¨ç›¸åŒã€‚\n",
    "\n",
    "def set_environment(seed):\n",
    "    torch.manual_seed(seed);\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed); random.seed(seed)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\n[ç’°å¢ƒè³‡è¨Š] éš¨æ©Ÿç¨®å­å·²è¨­å®šç‚º: {seed}\"); print(f\"[ç’°å¢ƒè³‡è¨Š] åŸ·è¡Œè¨­å‚™: {device}\")\n",
    "    if torch.cuda.is_available(): print(f\"[ç’°å¢ƒè³‡è¨Š] GPU åç¨±: {torch.cuda.get_device_name(0)}\")\n",
    "    return device\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, num_words=8000, oov_token=\"<OOV>\"): self.word_index, self.num_words, self.oov_token = {oov_token: 1}, num_words, oov_token\n",
    "    def fit_on_texts(self, texts):\n",
    "        word_counts = pd.Series([word for text in texts for word in text.split()]).value_counts()\n",
    "        for i, (word, _) in enumerate(word_counts.head(self.num_words - 2).items()): self.word_index[word] = i + 2\n",
    "    def texts_to_sequences(self, texts): return [[self.word_index.get(word, 1) for word in text.split()] for text in texts]\n",
    "\n",
    "class DashcamDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, config, transform, sampling_strategy, is_test=False):\n",
    "        self.df, self.tokenizer, self.config, self.transform, self.sampling_strategy, self.is_test = df, tokenizer, config, transform, sampling_strategy, is_test\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]; text_seq = self.tokenizer.texts_to_sequences([row['processed_text']])[0]\n",
    "        padded_text = np.zeros(self.config.MAX_TEXT_LEN, dtype=np.int64)\n",
    "        if len(text_seq) > 0:\n",
    "            if len(text_seq) > self.config.MAX_TEXT_LEN: text_seq = text_seq[-self.config.MAX_TEXT_LEN:]\n",
    "            padded_text[-len(text_seq):] = text_seq\n",
    "        image_sequence = torch.zeros(self.config.MAX_IMG_SEQ_LEN, 3, *self.config.IMG_SIZE)\n",
    "        all_paths = row['image_paths']\n",
    "        start_idx, end_idx = self.sampling_strategy['start'], self.sampling_strategy['end']\n",
    "        paths_to_load = all_paths[start_idx:end_idx] if len(all_paths) >= abs(start_idx) else []\n",
    "        for j, img_path in enumerate(paths_to_load):\n",
    "            if j >= self.config.MAX_IMG_SEQ_LEN: break\n",
    "            try: img = Image.open(img_path).convert('RGB'); image_sequence[j] = self.transform(img)\n",
    "            except Exception: continue\n",
    "        # é æ¸¬æ™‚åªè¿”å›æª”åï¼Œä¸è¿”å›æ¨™ç±¤\n",
    "        return torch.tensor(padded_text, dtype=torch.long), image_sequence, row['file_name']\n",
    "\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config; self.embedding = nn.Embedding(config.VOCAB_SIZE, config.EMBEDDING_DIM, padding_idx=0)\n",
    "        self.text_lstm = nn.LSTM(config.EMBEDDING_DIM, config.TEXT_LSTM_UNITS, batch_first=True, bidirectional=True); self.text_dropout = nn.Dropout(config.DROPOUT_RATE)\n",
    "        self.image_encoder = timm.create_model(config.MODEL_NAME, pretrained=False, num_classes=0, global_pool='avg') # pretrained=False, å› ç‚ºæˆ‘å€‘è¦è¼‰å…¥è‡ªå·±çš„æ¬Šé‡\n",
    "        self.image_lstm = nn.LSTM(self.image_encoder.num_features, config.IMAGE_LSTM_UNITS, batch_first=True, bidirectional=True); self.image_dropout = nn.Dropout(config.DROPOUT_RATE)\n",
    "        fusion_dim = (config.TEXT_LSTM_UNITS * 2) + (config.IMAGE_LSTM_UNITS * 2)\n",
    "        self.classifier = nn.Sequential(nn.Linear(fusion_dim, config.DENSE_UNITS), nn.ReLU(), nn.Dropout(config.DROPOUT_RATE), nn.Linear(config.DENSE_UNITS, 2))\n",
    "        print(f\"PyTorch å¤šæ¨¡æ…‹æ¨¡å‹ ({config.MODEL_NAME}) æ¶æ§‹å·²å»ºç«‹ã€‚\")\n",
    "    def forward(self, text, image):\n",
    "        text_embeds = self.embedding(text); _, (text_hidden, _) = self.text_lstm(text_embeds); text_features = torch.cat((text_hidden[-2,:,:], text_hidden[-1,:,:]), dim=1); text_features = self.text_dropout(text_features)\n",
    "        b, t, c, h, w = image.shape; img_flat = image.view(b * t, c, h, w); img_features = self.image_encoder(img_flat); img_features = img_features.view(b, t, -1); _, (img_hidden, _) = self.image_lstm(img_features); img_features = torch.cat((img_hidden[-2,:,:], img_hidden[-1,:,:]), dim=1); img_features = self.image_dropout(img_features)\n",
    "        return self.classifier(torch.cat((text_features, img_features), dim=1))\n",
    "\n",
    "# ===================================================================\n",
    "#                     *** 3. ä¸»é æ¸¬æµç¨‹ ***\n",
    "# ===================================================================\n",
    "def predict():\n",
    "    # --- è¨­å®š ---\n",
    "    config = Config()\n",
    "    device = set_environment(config.RANDOM_STATE)\n",
    "    \n",
    "    # æŒ‡å®šå·²è¨“ç·´æ¨¡å‹çš„æ ¹ç›®éŒ„\n",
    "    TRAINED_MODEL_DIR = 'output_pytorch_MultiWindow_20250710_131200'\n",
    "    if not os.path.isdir(TRAINED_MODEL_DIR):\n",
    "        print(f\"[éŒ¯èª¤] æ‰¾ä¸åˆ°è¨“ç·´å¥½çš„æ¨¡å‹è³‡æ–™å¤¾: {TRAINED_MODEL_DIR}\")\n",
    "        print(\"è«‹ç¢ºèªæ­¤è…³æœ¬èˆ‡è©²è³‡æ–™å¤¾ä½æ–¼åŒä¸€ç›®éŒ„ä¸‹ã€‚\")\n",
    "        return\n",
    "        \n",
    "    # å»ºç«‹ä¸€å€‹æ–°çš„è³‡æ–™å¤¾ä¾†å­˜æ”¾æœ¬æ¬¡é æ¸¬çš„çµæœ\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    INFERENCE_OUTPUT_DIR = f'inference_results_{TIMESTAMP}'\n",
    "    os.makedirs(INFERENCE_OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"æœ¬æ¬¡é æ¸¬çµæœå°‡å„²å­˜æ–¼: {INFERENCE_OUTPUT_DIR}\")\n",
    "\n",
    "    # --- é‡å»ºæ¡æ¨£ç­–ç•¥ ---\n",
    "    config.SAMPLING_STRATEGIES = []\n",
    "    for i in range(1, config.NUM_WINDOWS + 1):\n",
    "        start_offset = (i - 1) * config.WINDOW_SIZE + 1\n",
    "        end_offset = i * config.WINDOW_SIZE\n",
    "        config.SAMPLING_STRATEGIES.append({\n",
    "            'name': f'Last_{start_offset}_to_{end_offset}',\n",
    "            'start': -end_offset,\n",
    "            'end': None if i == 1 else -(end_offset - config.WINDOW_SIZE)\n",
    "        })\n",
    "    print(\"\\n\" + \"=\"*15, \"å·²é‡å»ºçš„æ¡æ¨£ç­–ç•¥\", \"=\"*15); [print(s) for s in config.SAMPLING_STRATEGIES]; print(\"-\" * 50)\n",
    "    \n",
    "    # --- æ­¥é©Ÿ A: æº–å‚™æ¸¬è©¦è³‡æ–™æ¡†æ¶ (èˆ‡è¨“ç·´æ™‚ç›¸åŒ) ---\n",
    "    print(\"\\n\" + \"=\"*15, \"æ­¥é©Ÿ A: æº–å‚™æ¸¬è©¦è³‡æ–™\", \"=\"*15)\n",
    "    def get_paths_and_counts(df, base_path_map):\n",
    "        img_paths_list = [sorted(glob(os.path.join(base_path_map['road' if row['file_name'].startswith('road') else 'freeway'], row['file_name'], '*.jpg'))) or sorted(glob(os.path.join(base_path_map['road' if row['file_name'].startswith('road') else 'freeway'], row['file_name'], '*.JPG'))) for _, row in df.iterrows()]\n",
    "        df['image_paths'] = img_paths_list; df['image_count'] = [len(p) for p in img_paths_list]\n",
    "        return df\n",
    "\n",
    "    test_captions_df = pd.concat([pd.read_csv('road_test_with_captions.csv'), pd.read_csv('freeway_test_with_captions.csv')], ignore_index=True)\n",
    "    sub_df = pd.read_csv('sample_submission.csv')\n",
    "    test_df = pd.merge(sub_df[['file_name']], test_captions_df, on='file_name', how='left')\n",
    "    test_base_path_map = {'road': os.path.join(config.BASE_IMG_PATH, 'road', 'test'), 'freeway': os.path.join(config.BASE_IMG_PATH, 'freeway', 'test')}\n",
    "    test_df = get_paths_and_counts(test_df, test_base_path_map)\n",
    "    def preprocess_text(text): return ' '.join([re.sub(r'[^a-z0-9\\s]', '', s.lower()).strip() for s in str(text).split('|')])\n",
    "    test_df['processed_text'] = test_df['captions'].apply(preprocess_text)\n",
    "    \n",
    "    # --- è¼‰å…¥ Tokenizer ---\n",
    "    tokenizer_path = os.path.join(TRAINED_MODEL_DIR, 'tokenizer.pkl')\n",
    "    try:\n",
    "        tokenizer = joblib.load(tokenizer_path)\n",
    "        print(\"Tokenizer å·²å¾ \" + tokenizer_path + \" è¼‰å…¥ã€‚\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[éŒ¯èª¤] æ‰¾ä¸åˆ° Tokenizer æª”æ¡ˆ: {tokenizer_path}\")\n",
    "        return\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # --- å®šç¾©åœ–åƒè½‰æ› ---\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(config.IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # --- æ­¥é©Ÿ B: é€²è¡Œé›†æˆé æ¸¬ ---\n",
    "    grand_ensemble_test_probs = []\n",
    "    \n",
    "    for strategy in config.SAMPLING_STRATEGIES:\n",
    "        strategy_name = strategy['name']\n",
    "        print(f\"\\n\" + \"#\"*25, f\"é–‹å§‹è™•ç†ç­–ç•¥: {strategy_name}\", \"#\"*25)\n",
    "        \n",
    "        STRATEGY_MODEL_DIR = os.path.join(TRAINED_MODEL_DIR, strategy_name)\n",
    "        \n",
    "        test_dataset = DashcamDataset(test_df, tokenizer, config, transform, sampling_strategy=strategy, is_test=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        \n",
    "        strategy_fold_probs = []\n",
    "        for fold in range(config.NUM_SPLITS):\n",
    "            # åœ¨è¨“ç·´è…³æœ¬ä¸­ï¼Œå…©éšæ®µè¨“ç·´æœ€çµ‚å„²å­˜çš„æ¨¡å‹åç¨±æ˜¯ temp_best_head_fold_{fold+1}.pth\n",
    "            # å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯å–®éšæ®µï¼Œè«‹å°‡ model_filename æ”¹ç‚º 'best_model_fold_{fold+1}.pth'\n",
    "            model_filename = f'temp_best_head_fold_{fold+1}.pth'\n",
    "            model_path = os.path.join(STRATEGY_MODEL_DIR, model_filename)\n",
    "\n",
    "            if not os.path.exists(model_path):\n",
    "                print(f\"[è­¦å‘Š] åœ¨ç­–ç•¥ [{strategy_name}] ä¸­æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ: {model_path}ï¼Œè·³éæ­¤ Foldã€‚\")\n",
    "                continue\n",
    "            \n",
    "            # è¼‰å…¥æ¨¡å‹\n",
    "            model = MultimodalModel(config).to(device)\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            model.eval()\n",
    "            \n",
    "            fold_probs = []\n",
    "            with torch.no_grad():\n",
    "                for text, image, _ in tqdm(test_loader, desc=f\"é æ¸¬ä¸­ (ç­–ç•¥: {strategy_name}, Fold: {fold+1})\"):\n",
    "                    outputs = model(text.to(device), image.to(device))\n",
    "                    # ä½¿ç”¨ softmax å–å¾—æ©Ÿç‡ï¼Œä¸¦é¸å– \"risk=1\" çš„æ©Ÿç‡\n",
    "                    probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "                    fold_probs.extend(probs)\n",
    "            \n",
    "            strategy_fold_probs.append(fold_probs)\n",
    "            del model; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "        if strategy_fold_probs:\n",
    "            # å°æ­¤ç­–ç•¥å…§çš„æ‰€æœ‰ Fold çµæœå–å¹³å‡\n",
    "            strategy_final_probs = np.mean(strategy_fold_probs, axis=0)\n",
    "            grand_ensemble_test_probs.append(strategy_final_probs)\n",
    "            print(f\"âœ… ç­–ç•¥ [{strategy_name}] é æ¸¬å®Œæˆã€‚\")\n",
    "        else:\n",
    "            print(f\"âŒ ç­–ç•¥ [{strategy_name}] æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹ï¼Œç„¡æ³•é€²è¡Œé æ¸¬ã€‚\")\n",
    "\n",
    "    # --- æ­¥é©Ÿ C: æœ€çµ‚é›†æˆèˆ‡å„²å­˜çµæœ ---\n",
    "    if not grand_ensemble_test_probs:\n",
    "        print(\"\\n\" + \"=\"*25, \"é æ¸¬å¤±æ•—\", \"=\"*25)\n",
    "        print(\"âŒ æœªèƒ½å¾ä»»ä½•ç­–ç•¥ä¸­æˆåŠŸè¼‰å…¥æ¨¡å‹ä¸¦ç”¢ç”Ÿé æ¸¬ã€‚è«‹æª¢æŸ¥ TRAINED_MODEL_DIR è·¯å¾‘èˆ‡æ¨¡å‹æª”æ¡ˆæ˜¯å¦å­˜åœ¨ã€‚\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*25, \"é–‹å§‹çµ‚æ¥µé›†æˆé æ¸¬\", \"=\"*25)\n",
    "        \n",
    "        # å°æ‰€æœ‰ç­–ç•¥çš„å¹³å‡çµæœå†æ¬¡å–å¹³å‡\n",
    "        grand_final_probs = np.mean(grand_ensemble_test_probs, axis=0)\n",
    "        grand_final_predictions = (grand_final_probs > 0.5).astype(int)\n",
    "        \n",
    "        # æº–å‚™æœ€çµ‚çš„ DataFrame\n",
    "        final_submission_df = pd.DataFrame({\n",
    "            'file_name': test_df['file_name'].tolist(),\n",
    "            'probability': grand_final_probs, # æ–°å¢çš„æ©Ÿç‡æ¬„ä½\n",
    "            'risk': grand_final_predictions\n",
    "        })\n",
    "        \n",
    "        # 1. å„²å­˜é‡ç¾çš„ submission æª” (åªæœ‰ file_name, risk)\n",
    "        repro_submission_path = os.path.join(INFERENCE_OUTPUT_DIR, 'submission_reproduced.csv')\n",
    "        final_submission_df[['file_name', 'risk']].to_csv(repro_submission_path, index=False)\n",
    "        print(f\"\\nâœ… é‡ç¾çš„é æ¸¬çµæœå·²å„²å­˜è‡³: {repro_submission_path}\")\n",
    "        print(\"é æ¸¬åˆ†ä½ˆ:\")\n",
    "        print(final_submission_df['risk'].value_counts())\n",
    "        \n",
    "        # 2. å„²å­˜åŒ…å«æ©Ÿç‡çš„ submission æª”\n",
    "        prob_submission_path = os.path.join(INFERENCE_OUTPUT_DIR, 'submission_probabilities.csv')\n",
    "        final_submission_df.to_csv(prob_submission_path, index=False)\n",
    "        print(f\"\\nâœ…âœ…âœ… åŒ…å«æ©Ÿç‡çš„é æ¸¬çµæœå·²å„²å­˜è‡³: {prob_submission_path}\")\n",
    "        \n",
    "    print(\"\\nğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰é æ¸¬æµç¨‹åŸ·è¡Œå®Œç•¢ï¼ğŸ‰ğŸ‰ğŸ‰\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c2bfd-01aa-46d5-8347-05a507208ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
