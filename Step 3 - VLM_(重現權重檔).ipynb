{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b57da3-eba5-41bf-8fc1-279f103d0440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[環境資訊] 隨機種子已設定為: 42\n",
      "[環境資訊] 執行設備: cuda\n",
      "[環境資訊] GPU 名稱: NVIDIA GeForce RTX 3080\n",
      "本次預測結果將儲存於: inference_results_20250719_001815\n",
      "\n",
      "=============== 已重建的採樣策略 ===============\n",
      "{'name': 'Last_1_to_5', 'start': -5, 'end': None}\n",
      "{'name': 'Last_6_to_10', 'start': -10, 'end': -5}\n",
      "{'name': 'Last_11_to_15', 'start': -15, 'end': -10}\n",
      "--------------------------------------------------\n",
      "\n",
      "=============== 步驟 A: 準備測試資料 ===============\n",
      "Tokenizer 已從 output_pytorch_MultiWindow_20250710_131200\\tokenizer.pkl 載入。\n",
      "--------------------------------------------------\n",
      "\n",
      "######################### 開始處理策略: Last_1_to_5 #########################\n",
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wolves9\\AppData\\Local\\Temp\\ipykernel_12856\\1005008649.py:210: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "預測中 (策略: Last_1_to_5, Fold: 1): 100%|█████████████████████████████████████████████| 31/31 [00:18<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預測中 (策略: Last_1_to_5, Fold: 2): 100%|█████████████████████████████████████████████| 31/31 [00:17<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預測中 (策略: Last_1_to_5, Fold: 3): 100%|█████████████████████████████████████████████| 31/31 [00:17<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預測中 (策略: Last_1_to_5, Fold: 4): 100%|█████████████████████████████████████████████| 31/31 [00:17<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預測中 (策略: Last_1_to_5, Fold: 5): 100%|█████████████████████████████████████████████| 31/31 [00:17<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 策略 [Last_1_to_5] 預測完成。\n",
      "\n",
      "######################### 開始處理策略: Last_6_to_10 #########################\n",
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預測中 (策略: Last_6_to_10, Fold: 1): 100%|████████████████████████████████████████████| 31/31 [00:17<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預測中 (策略: Last_6_to_10, Fold: 2): 100%|████████████████████████████████████████████| 31/31 [00:17<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預測中 (策略: Last_6_to_10, Fold: 3): 100%|████████████████████████████████████████████| 31/31 [00:18<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預測中 (策略: Last_6_to_10, Fold: 4): 100%|████████████████████████████████████████████| 31/31 [00:17<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預測中 (策略: Last_6_to_10, Fold: 5): 100%|████████████████████████████████████████████| 31/31 [00:17<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 策略 [Last_6_to_10] 預測完成。\n",
      "\n",
      "######################### 開始處理策略: Last_11_to_15 #########################\n",
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預測中 (策略: Last_11_to_15, Fold: 1): 100%|███████████████████████████████████████████| 31/31 [00:17<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預測中 (策略: Last_11_to_15, Fold: 2): 100%|███████████████████████████████████████████| 31/31 [00:17<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預測中 (策略: Last_11_to_15, Fold: 3): 100%|███████████████████████████████████████████| 31/31 [00:17<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預測中 (策略: Last_11_to_15, Fold: 4): 100%|███████████████████████████████████████████| 31/31 [00:17<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 多模態模型 (vit_base_patch16_clip_224.openai) 架構已建立。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預測中 (策略: Last_11_to_15, Fold: 5): 100%|███████████████████████████████████████████| 31/31 [00:17<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 策略 [Last_11_to_15] 預測完成。\n",
      "\n",
      "========================= 開始終極集成預測 =========================\n",
      "\n",
      "✅ 重現的預測結果已儲存至: inference_results_20250719_001815\\submission_reproduced.csv\n",
      "預測分佈:\n",
      "risk\n",
      "0    149\n",
      "1     94\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅✅✅ 包含機率的預測結果已儲存至: inference_results_20250719_001815\\submission_probabilities.csv\n",
      "\n",
      "🎉🎉🎉 所有預測流程執行完畢！🎉🎉🎉\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ===================================================================\n",
    "# 車禍預測比賽 - PyTorch 多模態模型預測腳本\n",
    "# 說明: 此腳本用於載入已訓練的模型，並對測試集進行預測。\n",
    "# ===================================================================\n",
    "\n",
    "# --- 0. 匯入必要套件 ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "# ===================================================================\n",
    "#                        *** 1. 設定區 ***\n",
    "# ===================================================================\n",
    "# !!! 重要 !!!\n",
    "# 此處的 Config 必須與您用來訓練的 VLM_5X3_2階段(065).py 腳本中的設定完全一致\n",
    "# 才能正確載入模型架構與處理資料。\n",
    "class Config:\n",
    "    # --- 1. 路徑、輸出與環境設定 ---\n",
    "    BASE_IMG_PATH = '.'                   # 圖像資料夾 (road, freeway) 的根路徑\n",
    "    RANDOM_STATE = 42                     # 固定隨機種子以確保實驗可重複\n",
    "    \n",
    "    # --- 2. 動態採樣策略設定 ---\n",
    "    WINDOW_SIZE = 5                       # 每個採樣窗口要包含多少張圖片\n",
    "    NUM_WINDOWS = 3                       # 總共要建立多少個採樣窗口 (組)\n",
    "    \n",
    "    # --- 3. 資料處理超參數 ---\n",
    "    MAX_IMG_SEQ_LEN = WINDOW_SIZE         # 圖像序列長度現在由窗口大小決定\n",
    "    MAX_TEXT_LEN = 512                    # VLM 文字序列的最大長度\n",
    "    IMG_SIZE = (224, 224)                 # 圖像模型的標準輸入尺寸\n",
    "    VOCAB_SIZE = 8000                     # 文字字典大小\n",
    "    \n",
    "    # --- 4. 模型架構超參數 ---\n",
    "    MODEL_NAME = 'vit_base_patch16_clip_224.openai' # 使用的預訓練圖像模型\n",
    "    EMBEDDING_DIM = 128                   # 文字嵌入層的維度\n",
    "    TEXT_LSTM_UNITS = 64                  # 處理文字的 LSTM 神經元數量\n",
    "    IMAGE_LSTM_UNITS = 64                 # 處理圖像序列的 LSTM 神經元數量\n",
    "    DENSE_UNITS = 128                     # 融合後的全連接層神經元數量\n",
    "    DROPOUT_RATE = 0.5                    # Dropout 層的比率\n",
    "    \n",
    "    # --- 5. 訓練過程超參數 (預測時用不到，但保留以示完整) ---\n",
    "    NUM_SPLITS = 5                        # 交叉驗證的摺數\n",
    "    BATCH_SIZE = 8                        # 批次大小 (預測時可以根據 VRAM 調整)\n",
    "\n",
    "# ===================================================================\n",
    "#                  *** 2. 輔助工具與類別定義 ***\n",
    "# ===================================================================\n",
    "# !!! 重要 !!!\n",
    "# 這些類別與函式也必須與原訓練腳本完全相同。\n",
    "\n",
    "def set_environment(seed):\n",
    "    torch.manual_seed(seed);\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed); random.seed(seed)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\n[環境資訊] 隨機種子已設定為: {seed}\"); print(f\"[環境資訊] 執行設備: {device}\")\n",
    "    if torch.cuda.is_available(): print(f\"[環境資訊] GPU 名稱: {torch.cuda.get_device_name(0)}\")\n",
    "    return device\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, num_words=8000, oov_token=\"<OOV>\"): self.word_index, self.num_words, self.oov_token = {oov_token: 1}, num_words, oov_token\n",
    "    def fit_on_texts(self, texts):\n",
    "        word_counts = pd.Series([word for text in texts for word in text.split()]).value_counts()\n",
    "        for i, (word, _) in enumerate(word_counts.head(self.num_words - 2).items()): self.word_index[word] = i + 2\n",
    "    def texts_to_sequences(self, texts): return [[self.word_index.get(word, 1) for word in text.split()] for text in texts]\n",
    "\n",
    "class DashcamDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, config, transform, sampling_strategy, is_test=False):\n",
    "        self.df, self.tokenizer, self.config, self.transform, self.sampling_strategy, self.is_test = df, tokenizer, config, transform, sampling_strategy, is_test\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]; text_seq = self.tokenizer.texts_to_sequences([row['processed_text']])[0]\n",
    "        padded_text = np.zeros(self.config.MAX_TEXT_LEN, dtype=np.int64)\n",
    "        if len(text_seq) > 0:\n",
    "            if len(text_seq) > self.config.MAX_TEXT_LEN: text_seq = text_seq[-self.config.MAX_TEXT_LEN:]\n",
    "            padded_text[-len(text_seq):] = text_seq\n",
    "        image_sequence = torch.zeros(self.config.MAX_IMG_SEQ_LEN, 3, *self.config.IMG_SIZE)\n",
    "        all_paths = row['image_paths']\n",
    "        start_idx, end_idx = self.sampling_strategy['start'], self.sampling_strategy['end']\n",
    "        paths_to_load = all_paths[start_idx:end_idx] if len(all_paths) >= abs(start_idx) else []\n",
    "        for j, img_path in enumerate(paths_to_load):\n",
    "            if j >= self.config.MAX_IMG_SEQ_LEN: break\n",
    "            try: img = Image.open(img_path).convert('RGB'); image_sequence[j] = self.transform(img)\n",
    "            except Exception: continue\n",
    "        # 預測時只返回檔名，不返回標籤\n",
    "        return torch.tensor(padded_text, dtype=torch.long), image_sequence, row['file_name']\n",
    "\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config; self.embedding = nn.Embedding(config.VOCAB_SIZE, config.EMBEDDING_DIM, padding_idx=0)\n",
    "        self.text_lstm = nn.LSTM(config.EMBEDDING_DIM, config.TEXT_LSTM_UNITS, batch_first=True, bidirectional=True); self.text_dropout = nn.Dropout(config.DROPOUT_RATE)\n",
    "        self.image_encoder = timm.create_model(config.MODEL_NAME, pretrained=False, num_classes=0, global_pool='avg') # pretrained=False, 因為我們要載入自己的權重\n",
    "        self.image_lstm = nn.LSTM(self.image_encoder.num_features, config.IMAGE_LSTM_UNITS, batch_first=True, bidirectional=True); self.image_dropout = nn.Dropout(config.DROPOUT_RATE)\n",
    "        fusion_dim = (config.TEXT_LSTM_UNITS * 2) + (config.IMAGE_LSTM_UNITS * 2)\n",
    "        self.classifier = nn.Sequential(nn.Linear(fusion_dim, config.DENSE_UNITS), nn.ReLU(), nn.Dropout(config.DROPOUT_RATE), nn.Linear(config.DENSE_UNITS, 2))\n",
    "        print(f\"PyTorch 多模態模型 ({config.MODEL_NAME}) 架構已建立。\")\n",
    "    def forward(self, text, image):\n",
    "        text_embeds = self.embedding(text); _, (text_hidden, _) = self.text_lstm(text_embeds); text_features = torch.cat((text_hidden[-2,:,:], text_hidden[-1,:,:]), dim=1); text_features = self.text_dropout(text_features)\n",
    "        b, t, c, h, w = image.shape; img_flat = image.view(b * t, c, h, w); img_features = self.image_encoder(img_flat); img_features = img_features.view(b, t, -1); _, (img_hidden, _) = self.image_lstm(img_features); img_features = torch.cat((img_hidden[-2,:,:], img_hidden[-1,:,:]), dim=1); img_features = self.image_dropout(img_features)\n",
    "        return self.classifier(torch.cat((text_features, img_features), dim=1))\n",
    "\n",
    "# ===================================================================\n",
    "#                     *** 3. 主預測流程 ***\n",
    "# ===================================================================\n",
    "def predict():\n",
    "    # --- 設定 ---\n",
    "    config = Config()\n",
    "    device = set_environment(config.RANDOM_STATE)\n",
    "    \n",
    "    # 指定已訓練模型的根目錄\n",
    "    TRAINED_MODEL_DIR = 'output_pytorch_MultiWindow_20250710_131200'\n",
    "    if not os.path.isdir(TRAINED_MODEL_DIR):\n",
    "        print(f\"[錯誤] 找不到訓練好的模型資料夾: {TRAINED_MODEL_DIR}\")\n",
    "        print(\"請確認此腳本與該資料夾位於同一目錄下。\")\n",
    "        return\n",
    "        \n",
    "    # 建立一個新的資料夾來存放本次預測的結果\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    INFERENCE_OUTPUT_DIR = f'inference_results_{TIMESTAMP}'\n",
    "    os.makedirs(INFERENCE_OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"本次預測結果將儲存於: {INFERENCE_OUTPUT_DIR}\")\n",
    "\n",
    "    # --- 重建採樣策略 ---\n",
    "    config.SAMPLING_STRATEGIES = []\n",
    "    for i in range(1, config.NUM_WINDOWS + 1):\n",
    "        start_offset = (i - 1) * config.WINDOW_SIZE + 1\n",
    "        end_offset = i * config.WINDOW_SIZE\n",
    "        config.SAMPLING_STRATEGIES.append({\n",
    "            'name': f'Last_{start_offset}_to_{end_offset}',\n",
    "            'start': -end_offset,\n",
    "            'end': None if i == 1 else -(end_offset - config.WINDOW_SIZE)\n",
    "        })\n",
    "    print(\"\\n\" + \"=\"*15, \"已重建的採樣策略\", \"=\"*15); [print(s) for s in config.SAMPLING_STRATEGIES]; print(\"-\" * 50)\n",
    "    \n",
    "    # --- 步驟 A: 準備測試資料框架 (與訓練時相同) ---\n",
    "    print(\"\\n\" + \"=\"*15, \"步驟 A: 準備測試資料\", \"=\"*15)\n",
    "    def get_paths_and_counts(df, base_path_map):\n",
    "        img_paths_list = [sorted(glob(os.path.join(base_path_map['road' if row['file_name'].startswith('road') else 'freeway'], row['file_name'], '*.jpg'))) or sorted(glob(os.path.join(base_path_map['road' if row['file_name'].startswith('road') else 'freeway'], row['file_name'], '*.JPG'))) for _, row in df.iterrows()]\n",
    "        df['image_paths'] = img_paths_list; df['image_count'] = [len(p) for p in img_paths_list]\n",
    "        return df\n",
    "\n",
    "    test_captions_df = pd.concat([pd.read_csv('road_test_with_captions.csv'), pd.read_csv('freeway_test_with_captions.csv')], ignore_index=True)\n",
    "    sub_df = pd.read_csv('sample_submission.csv')\n",
    "    test_df = pd.merge(sub_df[['file_name']], test_captions_df, on='file_name', how='left')\n",
    "    test_base_path_map = {'road': os.path.join(config.BASE_IMG_PATH, 'road', 'test'), 'freeway': os.path.join(config.BASE_IMG_PATH, 'freeway', 'test')}\n",
    "    test_df = get_paths_and_counts(test_df, test_base_path_map)\n",
    "    def preprocess_text(text): return ' '.join([re.sub(r'[^a-z0-9\\s]', '', s.lower()).strip() for s in str(text).split('|')])\n",
    "    test_df['processed_text'] = test_df['captions'].apply(preprocess_text)\n",
    "    \n",
    "    # --- 載入 Tokenizer ---\n",
    "    tokenizer_path = os.path.join(TRAINED_MODEL_DIR, 'tokenizer.pkl')\n",
    "    try:\n",
    "        tokenizer = joblib.load(tokenizer_path)\n",
    "        print(\"Tokenizer 已從 \" + tokenizer_path + \" 載入。\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[錯誤] 找不到 Tokenizer 檔案: {tokenizer_path}\")\n",
    "        return\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # --- 定義圖像轉換 ---\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(config.IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # --- 步驟 B: 進行集成預測 ---\n",
    "    grand_ensemble_test_probs = []\n",
    "    \n",
    "    for strategy in config.SAMPLING_STRATEGIES:\n",
    "        strategy_name = strategy['name']\n",
    "        print(f\"\\n\" + \"#\"*25, f\"開始處理策略: {strategy_name}\", \"#\"*25)\n",
    "        \n",
    "        STRATEGY_MODEL_DIR = os.path.join(TRAINED_MODEL_DIR, strategy_name)\n",
    "        \n",
    "        test_dataset = DashcamDataset(test_df, tokenizer, config, transform, sampling_strategy=strategy, is_test=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        \n",
    "        strategy_fold_probs = []\n",
    "        for fold in range(config.NUM_SPLITS):\n",
    "            # 在訓練腳本中，兩階段訓練最終儲存的模型名稱是 temp_best_head_fold_{fold+1}.pth\n",
    "            # 如果您使用的是單階段，請將 model_filename 改為 'best_model_fold_{fold+1}.pth'\n",
    "            model_filename = f'temp_best_head_fold_{fold+1}.pth'\n",
    "            model_path = os.path.join(STRATEGY_MODEL_DIR, model_filename)\n",
    "\n",
    "            if not os.path.exists(model_path):\n",
    "                print(f\"[警告] 在策略 [{strategy_name}] 中找不到模型檔案: {model_path}，跳過此 Fold。\")\n",
    "                continue\n",
    "            \n",
    "            # 載入模型\n",
    "            model = MultimodalModel(config).to(device)\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            model.eval()\n",
    "            \n",
    "            fold_probs = []\n",
    "            with torch.no_grad():\n",
    "                for text, image, _ in tqdm(test_loader, desc=f\"預測中 (策略: {strategy_name}, Fold: {fold+1})\"):\n",
    "                    outputs = model(text.to(device), image.to(device))\n",
    "                    # 使用 softmax 取得機率，並選取 \"risk=1\" 的機率\n",
    "                    probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "                    fold_probs.extend(probs)\n",
    "            \n",
    "            strategy_fold_probs.append(fold_probs)\n",
    "            del model; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "        if strategy_fold_probs:\n",
    "            # 對此策略內的所有 Fold 結果取平均\n",
    "            strategy_final_probs = np.mean(strategy_fold_probs, axis=0)\n",
    "            grand_ensemble_test_probs.append(strategy_final_probs)\n",
    "            print(f\"✅ 策略 [{strategy_name}] 預測完成。\")\n",
    "        else:\n",
    "            print(f\"❌ 策略 [{strategy_name}] 沒有找到任何模型，無法進行預測。\")\n",
    "\n",
    "    # --- 步驟 C: 最終集成與儲存結果 ---\n",
    "    if not grand_ensemble_test_probs:\n",
    "        print(\"\\n\" + \"=\"*25, \"預測失敗\", \"=\"*25)\n",
    "        print(\"❌ 未能從任何策略中成功載入模型並產生預測。請檢查 TRAINED_MODEL_DIR 路徑與模型檔案是否存在。\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*25, \"開始終極集成預測\", \"=\"*25)\n",
    "        \n",
    "        # 對所有策略的平均結果再次取平均\n",
    "        grand_final_probs = np.mean(grand_ensemble_test_probs, axis=0)\n",
    "        grand_final_predictions = (grand_final_probs > 0.5).astype(int)\n",
    "        \n",
    "        # 準備最終的 DataFrame\n",
    "        final_submission_df = pd.DataFrame({\n",
    "            'file_name': test_df['file_name'].tolist(),\n",
    "            'probability': grand_final_probs, # 新增的機率欄位\n",
    "            'risk': grand_final_predictions\n",
    "        })\n",
    "        \n",
    "        # 1. 儲存重現的 submission 檔 (只有 file_name, risk)\n",
    "        repro_submission_path = os.path.join(INFERENCE_OUTPUT_DIR, 'submission_reproduced.csv')\n",
    "        final_submission_df[['file_name', 'risk']].to_csv(repro_submission_path, index=False)\n",
    "        print(f\"\\n✅ 重現的預測結果已儲存至: {repro_submission_path}\")\n",
    "        print(\"預測分佈:\")\n",
    "        print(final_submission_df['risk'].value_counts())\n",
    "        \n",
    "        # 2. 儲存包含機率的 submission 檔\n",
    "        prob_submission_path = os.path.join(INFERENCE_OUTPUT_DIR, 'submission_probabilities.csv')\n",
    "        final_submission_df.to_csv(prob_submission_path, index=False)\n",
    "        print(f\"\\n✅✅✅ 包含機率的預測結果已儲存至: {prob_submission_path}\")\n",
    "        \n",
    "    print(\"\\n🎉🎉🎉 所有預測流程執行完畢！🎉🎉🎉\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c2bfd-01aa-46d5-8347-05a507208ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
